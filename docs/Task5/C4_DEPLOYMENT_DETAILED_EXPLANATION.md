# Task 5: Deployment Diagram - –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ AI-driven production –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã

## üéØ –¶–µ–ª—å –¥–∏–∞–≥—Ä–∞–º–º—ã

Deployment –¥–∏–∞–≥—Ä–∞–º–º–∞ Task 5 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç **production-ready —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ AI-driven —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã**, –ø–æ–∫–∞–∑—ã–≤–∞—è –∫–∞–∫ ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç—Å—è —Å –æ–±–ª–∞—á–Ω–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –î–∏–∞–≥—Ä–∞–º–º–∞ —Å–ª—É–∂–∏—Ç —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º –¥–ª—è DevOps –∫–æ–º–∞–Ω–¥ –ø–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –≤ production.

## ‚òÅÔ∏è AWS Cloud AI Platform: –û–±–ª–∞—á–Ω–∞—è ML –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞

### Production VPC AI - –°–µ—Ç–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å AI –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π

#### Terraform –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è AI-enhanced VPC
```hcl
# infrastructure/terraform/vpc-ai.tf
resource "aws_vpc" "production_vpc_ai" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name        = "production-vpc-ai"
    Environment = "production"
    AIEnabled   = "true"
    Project     = "auto-ru-federation-ai"
  }
}

# –ü–æ–¥—Å–µ—Ç–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö AI –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
resource "aws_subnet" "public_subnet_ai" {
  count             = 2
  vpc_id            = aws_vpc.production_vpc_ai.id
  cidr_block        = "10.0.${count.index + 1}.0/24"
  availability_zone = data.aws_availability_zones.available.names[count.index]
  
  map_public_ip_on_launch = true

  tags = {
    Name = "public-subnet-ai-${count.index + 1}"
    Type = "public"
    AIWorkload = "gateway-inference"
  }
}

resource "aws_subnet" "private_subnet_ai" {
  count             = 3
  vpc_id            = aws_vpc.production_vpc_ai.id
  cidr_block        = "10.0.${count.index + 10}.0/24"
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = "private-subnet-ai-${count.index + 1}"
    Type = "private"
    AIWorkload = count.index == 0 ? "ml-inference" : count.index == 1 ? "ml-training" : "data-processing"
  }
}

# NAT Gateway –¥–ª—è –ø—Ä–∏–≤–∞—Ç–Ω—ã—Ö –ø–æ–¥—Å–µ—Ç–µ–π —Å AI —Ç—Ä–∞—Ñ–∏–∫–æ–º
resource "aws_nat_gateway" "ai_nat_gateway" {
  count         = 2
  allocation_id = aws_eip.ai_nat_eip[count.index].id
  subnet_id     = aws_subnet.public_subnet_ai[count.index].id

  tags = {
    Name = "ai-nat-gateway-${count.index + 1}"
    AIOptimized = "true"
  }
}

# Security Groups –¥–ª—è AI –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
resource "aws_security_group" "ai_gateway_sg" {
  name_description = "Security group for AI Gateway"
  vpc_id          = aws_vpc.production_vpc_ai.id

  # HTTP/HTTPS —Ç—Ä–∞—Ñ–∏–∫
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # GraphQL –ø–æ—Ä—Ç
  ingress {
    from_port   = 4000
    to_port     = 4000
    protocol    = "tcp"
    cidr_blocks = [aws_vpc.production_vpc_ai.cidr_block]
  }

  # ML inference –ø–æ—Ä—Ç—ã
  ingress {
    from_port   = 8080
    to_port     = 8090
    protocol    = "tcp"
    cidr_blocks = [aws_vpc.production_vpc_ai.cidr_block]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "ai-gateway-sg"
    Component = "ai-gateway"
  }
}
```

### Intelligent ALB - ML-enhanced Load Balancer

#### Application Load Balancer —Å AI –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π
```yaml
# kubernetes/manifests/alb-ai-controller.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alb-ai-config
  namespace: kube-system
data:
  ai-routing-config.json: |
    {
      "mlRoutingEnabled": true,
      "performancePredictionModel": {
        "endpoint": "http://model-server:8080/predict-latency",
        "timeout": 100,
        "fallbackStrategy": "round-robin"
      },
      "adaptiveWeights": {
        "enabled": true,
        "learningRate": 0.01,
        "updateInterval": 30
      },
      "healthCheckAI": {
        "enabled": true,
        "anomalyDetection": true,
        "predictiveFailover": true
      }
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alb-ai-controller
  namespace: kube-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app: alb-ai-controller
  template:
    metadata:
      labels:
        app: alb-ai-controller
    spec:
      containers:
      - name: alb-ai-controller
        image: auto-ru/alb-ai-controller:v1.0.0
        ports:
        - containerPort: 8080
        env:
        - name: AWS_REGION
          value: "us-east-1"
        - name: ML_MODEL_ENDPOINT
          value: "http://model-server:8080"
        - name: PROMETHEUS_ENDPOINT
          value: "http://prometheus:9090"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
```

## üöÄ EKS AI Cluster: Kubernetes —Å AI –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º–∏

### AI-Enhanced Kubernetes Configuration

#### EKS –∫–ª–∞—Å—Ç–µ—Ä —Å ML –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
```yaml
# kubernetes/cluster/eks-ai-cluster.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: auto-ru-ai-cluster
  region: us-east-1
  version: "1.28"

vpc:
  id: vpc-ai-production
  subnets:
    private:
      us-east-1a: { id: subnet-ai-private-1 }
      us-east-1b: { id: subnet-ai-private-2 }
      us-east-1c: { id: subnet-ai-private-3 }
    public:
      us-east-1a: { id: subnet-ai-public-1 }
      us-east-1b: { id: subnet-ai-public-2 }

# Node groups –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö AI workloads
nodeGroups:
  # Gateway nodes - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è inference
  - name: ai-gateway-nodes
    instanceType: c5.2xlarge
    desiredCapacity: 3
    minSize: 2
    maxSize: 10
    volumeSize: 100
    volumeType: gp3
    labels:
      workload-type: "ai-gateway"
      node-class: "inference-optimized"
    taints:
      - key: "ai-gateway"
        value: "true"
        effect: "NoSchedule"
    tags:
      AIWorkload: "gateway-inference"
      AutoScaling: "enabled"

  # ML inference nodes - GPU enabled
  - name: ml-inference-nodes
    instanceType: g4dn.xlarge
    desiredCapacity: 2
    minSize: 1
    maxSize: 5
    volumeSize: 200
    volumeType: gp3
    labels:
      workload-type: "ml-inference"
      node-class: "gpu-enabled"
      nvidia.com/gpu: "true"
    taints:
      - key: "ml-inference"
        value: "true"
        effect: "NoSchedule"
    tags:
      AIWorkload: "ml-inference"
      GPUEnabled: "true"

  # Training nodes - –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ
  - name: ml-training-nodes
    instanceType: p3.2xlarge
    desiredCapacity: 0
    minSize: 0
    maxSize: 3
    volumeSize: 500
    volumeType: gp3
    labels:
      workload-type: "ml-training"
      node-class: "training-optimized"
    taints:
      - key: "ml-training"
        value: "true"
        effect: "NoSchedule"
    tags:
      AIWorkload: "ml-training"
      SpotInstance: "true"

# Addons –¥–ª—è AI workloads
addons:
  - name: vpc-cni
    version: latest
  - name: coredns
    version: latest
  - name: kube-proxy
    version: latest
  - name: aws-ebs-csi-driver
    version: latest
  - name: nvidia-device-plugin
    version: latest

# IAM —Ä–æ–ª–∏ –¥–ª—è AI —Å–µ—Ä–≤–∏—Å–æ–≤
iam:
  withOIDC: true
  serviceAccounts:
    - metadata:
        name: ai-gateway-sa
        namespace: default
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess
        - arn:aws:iam::aws:policy/AmazonSageMakerReadOnly
      tags:
        Component: "ai-gateway"
    
    - metadata:
        name: ml-inference-sa
        namespace: default
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
      tags:
        Component: "ml-inference"

# CloudWatch logging –¥–ª—è AI –º–µ—Ç—Ä–∏–∫
cloudWatch:
  clusterLogging:
    enableTypes: ["api", "audit", "authenticator", "controllerManager", "scheduler"]
    logRetentionInDays: 30
```

### Apollo Gateway AI Pod - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π Gateway

#### Kubernetes Deployment —Å ML –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
```yaml
# kubernetes/deployments/apollo-gateway-ai.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apollo-gateway-ai
  namespace: default
  labels:
    app: apollo-gateway-ai
    component: ai-gateway
    version: v1.0.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: apollo-gateway-ai
  template:
    metadata:
      labels:
        app: apollo-gateway-ai
        component: ai-gateway
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "4000"
        prometheus.io/path: "/metrics"
    spec:
      nodeSelector:
        workload-type: "ai-gateway"
      tolerations:
      - key: "ai-gateway"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      
      serviceAccountName: ai-gateway-sa
      
      initContainers:
      # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –º–æ–¥–µ–ª–µ–π
      - name: model-downloader
        image: auto-ru/model-downloader:v1.0.0
        command:
        - /bin/sh
        - -c
        - |
          echo "Downloading ML models..."
          aws s3 sync s3://auto-ru-ml-models/gateway-ai/ /models/
          echo "Models downloaded successfully"
        volumeMounts:
        - name: ml-models
          mountPath: /models
        env:
        - name: AWS_REGION
          value: "us-east-1"
      
      containers:
      - name: apollo-gateway-ai
        image: auto-ru/apollo-gateway-ai:v1.0.0
        ports:
        - containerPort: 4000
          name: graphql
        - containerPort: 8080
          name: metrics
        
        env:
        # GraphQL –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        - name: GRAPHQL_PORT
          value: "4000"
        - name: GRAPHQL_PATH
          value: "/graphql"
        
        # ML –º–æ–¥–µ–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        - name: ML_MODELS_PATH
          value: "/models"
        - name: PERFORMANCE_PREDICTOR_MODEL
          value: "/models/performance-predictor.json"
        - name: QUERY_CLASSIFIER_MODEL
          value: "/models/query-classifier.json"
        - name: ROUTING_OPTIMIZER_MODEL
          value: "/models/routing-optimizer.json"
        
        # AI —Å–µ—Ä–≤–∏—Å—ã endpoints
        - name: MODEL_SERVER_ENDPOINT
          value: "http://model-server:8080"
        - name: FEATURE_STORE_ENDPOINT
          value: "http://feature-store:8080"
        
        # Subgraph endpoints
        - name: USER_SUBGRAPH_URL
          value: "http://user-subgraph-ai:4001/graphql"
        - name: OFFER_SUBGRAPH_URL
          value: "http://offer-subgraph-ai:4002/graphql"
        - name: REVIEW_SUBGRAPH_URL
          value: "http://review-subgraph-ai:4003/graphql"
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        - name: LOG_LEVEL
          value: "info"
        - name: ENABLE_TRACING
          value: "true"
        - name: JAEGER_ENDPOINT
          value: "http://jaeger-collector:14268/api/traces"
        
        # AI –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        - name: AI_ENABLED
          value: "true"
        - name: ML_INFERENCE_TIMEOUT
          value: "100"
        - name: ADAPTIVE_ROUTING_ENABLED
          value: "true"
        - name: PERFORMANCE_PREDICTION_ENABLED
          value: "true"
        
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        
        volumeMounts:
        - name: ml-models
          mountPath: /models
          readOnly: true
        - name: config
          mountPath: /app/config
          readOnly: true
        
        livenessProbe:
          httpGet:
            path: /health
            port: 4000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 4000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 2
        
        # Graceful shutdown
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - |
                echo "Gracefully shutting down Apollo Gateway AI..."
                # –î–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                sleep 15
      
      volumes:
      - name: ml-models
        emptyDir: {}
      - name: config
        configMap:
          name: apollo-gateway-ai-config

---
apiVersion: v1
kind: Service
metadata:
  name: apollo-gateway-ai
  labels:
    app: apollo-gateway-ai
spec:
  type: ClusterIP
  ports:
  - port: 4000
    targetPort: 4000
    name: graphql
  - port: 8080
    targetPort: 8080
    name: metrics
  selector:
    app: apollo-gateway-ai

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: apollo-gateway-ai-config
data:
  gateway-config.json: |
    {
      "ai": {
        "enabled": true,
        "models": {
          "performancePredictor": {
            "path": "/models/performance-predictor.json",
            "timeout": 100,
            "cacheSize": 1000
          },
          "queryClassifier": {
            "path": "/models/query-classifier.json",
            "timeout": 50,
            "cacheSize": 2000
          },
          "routingOptimizer": {
            "path": "/models/routing-optimizer.json",
            "learningRate": 0.01,
            "updateInterval": 300
          }
        },
        "features": {
          "adaptiveRouting": true,
          "performancePrediction": true,
          "queryOptimization": true,
          "anomalyDetection": true
        }
      },
      "federation": {
        "introspectionEnabled": false,
        "queryPlanCaching": true,
        "subscriptions": false
      },
      "caching": {
        "enabled": true,
        "redis": {
          "host": "redis-ai-primary",
          "port": 6379,
          "db": 0
        },
        "ttl": {
          "default": 300,
          "query": 60,
          "schema": 3600
        }
      }
    }
```

### Model Server - ML –º–æ–¥–µ–ª–∏ –≤ production

#### TorchServe deployment –¥–ª—è ML inference
```yaml
# kubernetes/deployments/model-server.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-server
  namespace: default
  labels:
    app: model-server
    component: ml-inference
spec:
  replicas: 2
  selector:
    matchLabels:
      app: model-server
  template:
    metadata:
      labels:
        app: model-server
        component: ml-inference
    spec:
      nodeSelector:
        workload-type: "ml-inference"
      tolerations:
      - key: "ml-inference"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      
      serviceAccountName: ml-inference-sa
      
      initContainers:
      - name: model-loader
        image: auto-ru/model-loader:v1.0.0
        command:
        - /bin/sh
        - -c
        - |
          echo "Loading ML models from S3..."
          aws s3 sync s3://auto-ru-ml-models/inference/ /models/
          
          # –°–æ–∑–¥–∞–Ω–∏–µ model store –¥–ª—è TorchServe
          torch-model-archiver --model-name performance_predictor \
            --version 1.0 \
            --model-file /models/performance_predictor.py \
            --serialized-file /models/performance_predictor.pth \
            --handler /models/performance_predictor_handler.py \
            --export-path /model-store/
          
          torch-model-archiver --model-name query_classifier \
            --version 1.0 \
            --model-file /models/query_classifier.py \
            --serialized-file /models/query_classifier.pth \
            --handler /models/query_classifier_handler.py \
            --export-path /model-store/
          
          echo "Models loaded and archived successfully"
        volumeMounts:
        - name: model-store
          mountPath: /model-store
        - name: models-cache
          mountPath: /models
      
      containers:
      - name: torchserve
        image: pytorch/torchserve:0.8.2-gpu
        ports:
        - containerPort: 8080
          name: inference
        - containerPort: 8081
          name: management
        - containerPort: 8082
          name: metrics
        
        env:
        - name: TS_CONFIG_FILE
          value: "/config/config.properties"
        
        command:
        - torchserve
        - --start
        - --model-store=/model-store
        - --models=performance_predictor=performance_predictor.mar
        - --models=query_classifier=query_classifier.mar
        - --ts-config=/config/config.properties
        
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
            nvidia.com/gpu: 1
          limits:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
        
        volumeMounts:
        - name: model-store
          mountPath: /model-store
          readOnly: true
        - name: torchserve-config
          mountPath: /config
          readOnly: true
        - name: logs
          mountPath: /logs
        
        livenessProbe:
          httpGet:
            path: /ping
            port: 8080
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /ping
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
      
      # Sidecar –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –º–æ–¥–µ–ª–µ–π
      - name: model-monitor
        image: auto-ru/model-monitor:v1.0.0
        ports:
        - containerPort: 9090
          name: monitor-metrics
        
        env:
        - name: TORCHSERVE_MANAGEMENT_API
          value: "http://localhost:8081"
        - name: PROMETHEUS_ENDPOINT
          value: "http://prometheus:9090"
        - name: MODEL_DRIFT_THRESHOLD
          value: "0.1"
        - name: PERFORMANCE_THRESHOLD
          value: "0.05"
        
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
        
        volumeMounts:
        - name: logs
          mountPath: /logs
          readOnly: true
      
      volumes:
      - name: model-store
        emptyDir: {}
      - name: models-cache
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: torchserve-config
        configMap:
          name: torchserve-config

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: torchserve-config
data:
  config.properties: |
    inference_address=http://0.0.0.0:8080
    management_address=http://0.0.0.0:8081
    metrics_address=http://0.0.0.0:8082
    
    # Performance settings
    number_of_netty_threads=8
    job_queue_size=1000
    number_of_gpu=1
    
    # Model settings
    default_workers_per_model=2
    max_workers=4
    batch_size=8
    max_batch_delay=100
    
    # Logging
    default_response_timeout=120
    unregister_model_timeout=120
    decode_input_request=true
    
    # Metrics
    enable_metrics_api=true
    metrics_format=prometheus
    
    # CORS
    cors_allowed_origin=*
    cors_allowed_methods=GET,POST,PUT,DELETE
    cors_allowed_headers=*
```

## üóÑÔ∏è Intelligent Storage Layer: AI-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

### PostgreSQL AI - ML-enhanced –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö

#### RDS PostgreSQL —Å ML —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏
```yaml
# kubernetes/statefulsets/postgres-ai.yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-ai-cluster
  namespace: default
spec:
  instances: 3
  
  # PostgreSQL –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å ML —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏
  postgresql:
    parameters:
      # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
      shared_preload_libraries: "pg_stat_statements,auto_explain,pg_ml"
      max_connections: "200"
      shared_buffers: "256MB"
      effective_cache_size: "1GB"
      work_mem: "4MB"
      maintenance_work_mem: "64MB"
      
      # ML-specific –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
      pg_ml.enabled: "on"
      pg_ml.model_cache_size: "128MB"
      pg_ml.inference_timeout: "5s"
      
      # Query optimization
      auto_explain.log_min_duration: "1s"
      auto_explain.log_analyze: "on"
      auto_explain.log_buffers: "on"
      
      # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è ML –∞–Ω–∞–ª–∏–∑–∞
      pg_stat_statements.track: "all"
      pg_stat_statements.max: "10000"
      pg_stat_statements.save: "on"
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
    initdb:
      database: auto_ru_ai
      owner: auto_ru_user
      secret:
        name: postgres-credentials
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    databases:
      - name: ml_features
        owner: auto_ru_user
      - name: model_metadata
        owner: auto_ru_user
  
  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
  monitoring:
    enabled: true
    prometheusRule:
      enabled: true
    
  # Backup –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
  backup:
    retentionPolicy: "30d"
    barmanObjectStore:
      destinationPath: "s3://auto-ru-postgres-backups"
      s3Credentials:
        accessKeyId:
          name: postgres-backup-credentials
          key: ACCESS_KEY_ID
        secretAccessKey:
          name: postgres-backup-credentials
          key: SECRET_ACCESS_KEY
      wal:
        retention: "7d"
      data:
        retention: "30d"
  
  # –†–µ—Å—É—Ä—Å—ã
  resources:
    requests:
      memory: "2Gi"
      cpu: "1000m"
    limits:
      memory: "4Gi"
      cpu: "2000m"
  
  # –•—Ä–∞–Ω–∏–ª–∏—â–µ
  storage:
    size: "100Gi"
    storageClass: "gp3-encrypted"
  
  # Affinity –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ AZ
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            cnpg.io/cluster: postgres-ai-cluster
        topologyKey: topology.kubernetes.io/zone

---
# ML —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-ml-functions
data:
  ml-functions.sql: |
    -- –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã –¥–ª—è ML —Ñ—É–Ω–∫—Ü–∏–π
    CREATE SCHEMA IF NOT EXISTS ml_ops;
    
    -- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
    CREATE OR REPLACE FUNCTION ml_ops.predict_query_performance(
        query_text TEXT,
        query_params JSONB DEFAULT '{}'::jsonb
    ) RETURNS TABLE(
        estimated_duration_ms FLOAT,
        estimated_memory_mb FLOAT,
        confidence_score FLOAT
    ) AS $$
    BEGIN
        -- –í—ã–∑–æ–≤ ML –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ pg_ml —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        RETURN QUERY
        SELECT 
            (ml_predict('query_performance_model', 
                        ml_ops.extract_query_features(query_text, query_params))).duration_ms,
            (ml_predict('query_performance_model', 
                        ml_ops.extract_query_features(query_text, query_params))).memory_mb,
            (ml_predict('query_performance_model', 
                        ml_ops.extract_query_features(query_text, query_params))).confidence;
    END;
    $$ LANGUAGE plpgsql;
    
    -- –§—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    CREATE OR REPLACE FUNCTION ml_ops.extract_query_features(
        query_text TEXT,
        query_params JSONB DEFAULT '{}'::jsonb
    ) RETURNS FLOAT[] AS $$
    DECLARE
        features FLOAT[];
        query_plan JSONB;
    BEGIN
        -- –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–ª–∞–Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        EXECUTE 'EXPLAIN (FORMAT JSON) ' || query_text 
        INTO query_plan;
        
        -- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –ø–ª–∞–Ω–∞
        features := ARRAY[
            -- –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            (query_plan->'Plan'->>'Total Cost')::FLOAT,
            (query_plan->'Plan'->>'Plan Rows')::FLOAT,
            (query_plan->'Plan'->>'Plan Width')::FLOAT,
            
            -- –°–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
            ml_ops.calculate_query_complexity(query_text),
            ml_ops.count_joins(query_text),
            ml_ops.count_subqueries(query_text),
            
            -- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            jsonb_array_length(COALESCE(query_params, '{}'::jsonb))::FLOAT
        ];
        
        RETURN features;
    END;
    $$ LANGUAGE plpgsql;
    
    -- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ ML
    CREATE OR REPLACE FUNCTION ml_ops.suggest_indexes()
    RETURNS TABLE(
        table_name TEXT,
        column_names TEXT[],
        index_type TEXT,
        expected_improvement FLOAT
    ) AS $$
    BEGIN
        -- –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤
        RETURN QUERY
        WITH query_stats AS (
            SELECT 
                schemaname,
                tablename,
                attname,
                n_distinct,
                correlation,
                most_common_vals
            FROM pg_stats
            WHERE schemaname NOT IN ('information_schema', 'pg_catalog')
        ),
        ml_suggestions AS (
            SELECT 
                qs.tablename,
                ARRAY[qs.attname] as columns,
                CASE 
                    WHEN qs.n_distinct > 1000 THEN 'btree'
                    WHEN qs.correlation < 0.1 THEN 'hash'
                    ELSE 'btree'
                END as idx_type,
                ml_predict('index_performance_model', 
                          ARRAY[qs.n_distinct, qs.correlation])::FLOAT as improvement
            FROM query_stats qs
            WHERE NOT EXISTS (
                SELECT 1 FROM pg_indexes pi 
                WHERE pi.tablename = qs.tablename 
                AND pi.indexdef LIKE '%' || qs.attname || '%'
            )
        )
        SELECT 
            ms.tablename,
            ms.columns,
            ms.idx_type,
            ms.improvement
        FROM ml_suggestions ms
        WHERE ms.improvement > 0.1
        ORDER BY ms.improvement DESC;
    END;
    $$ LANGUAGE plpgsql;
```

### Redis AI - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ

#### Redis —Å RedisAI –¥–ª—è ML inference
```yaml
# kubernetes/deployments/redis-ai.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-ai-primary
  namespace: default
spec:
  serviceName: redis-ai-primary
  replicas: 1
  selector:
    matchLabels:
      app: redis-ai-primary
  template:
    metadata:
      labels:
        app: redis-ai-primary
        component: cache-ai
    spec:
      containers:
      - name: redis-ai
        image: redislabs/redisai:1.2.7-gpu
        ports:
        - containerPort: 6379
          name: redis
        
        command:
        - redis-server
        - /config/redis.conf
        
        env:
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: redis-credentials
              key: password
        
        resources:
          requests:
            memory: "2Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "1000m"
        
        volumeMounts:
        - name: redis-config
          mountPath: /config
        - name: redis-data
          mountPath: /data
        - name: ai-models
          mountPath: /models
        
        livenessProbe:
          exec:
            command:
            - redis-cli
            - --no-auth-warning
            - -a
            - $(REDIS_PASSWORD)
            - ping
          initialDelaySeconds: 30
          periodSeconds: 10
        
        readinessProbe:
          exec:
            command:
            - redis-cli
            - --no-auth-warning
            - -a
            - $(REDIS_PASSWORD)
            - ping
          initialDelaySeconds: 5
          periodSeconds: 5
      
      # Sidecar –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ AI –º–æ–¥–µ–ª–µ–π
      - name: model-loader
        image: auto-ru/redis-ai-loader:v1.0.0
        command:
        - /bin/sh
        - -c
        - |
          echo "Loading AI models into Redis..."
          
          # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è TTL
          redis-cli -h localhost -a $REDIS_PASSWORD \
            AI.MODELSTORE cache_ttl_predictor TF CPU \
            BLOB < /models/cache_ttl_predictor.pb
          
          # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–æ—Å—Ç—É–ø–∞
          redis-cli -h localhost -a $REDIS_PASSWORD \
            AI.MODELSTORE access_pattern_predictor TF CPU \
            BLOB < /models/access_pattern_predictor.pb
          
          # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–µ—à–∞
          redis-cli -h localhost -a $REDIS_PASSWORD \
            AI.MODELSTORE cache_personalization TF CPU \
            BLOB < /models/cache_personalization.pb
          
          echo "AI models loaded successfully"
          
          # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ø–µ—Ä–µ–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
          while true; do
            sleep 3600  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–π —á–∞—Å
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π –º–æ–¥–µ–ª–µ–π –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
          done
        
        env:
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: redis-credentials
              key: password
        
        volumeMounts:
        - name: ai-models
          mountPath: /models
        
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
      
      initContainers:
      - name: model-downloader
        image: auto-ru/model-downloader:v1.0.0
        command:
        - /bin/sh
        - -c
        - |
          echo "Downloading Redis AI models..."
          aws s3 sync s3://auto-ru-ml-models/redis-ai/ /models/
          echo "Models downloaded successfully"
        
        volumeMounts:
        - name: ai-models
          mountPath: /models
      
      volumes:
      - name: redis-config
        configMap:
          name: redis-ai-config
      - name: ai-models
        emptyDir: {}
  
  volumeClaimTemplates:
  - metadata:
      name: redis-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "gp3-encrypted"
      resources:
        requests:
          storage: 50Gi

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-ai-config
data:
  redis.conf: |
    # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Redis
    bind 0.0.0.0
    port 6379
    requirepass ${REDIS_PASSWORD}
    
    # –ü–∞–º—è—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    maxmemory 3gb
    maxmemory-policy allkeys-lru
    
    # –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
    save 900 1
    save 300 10
    save 60 10000
    
    # RedisAI –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    loadmodule /usr/lib/redis/modules/redisai.so
    
    # AI-specific –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    # –†–∞–∑–º–µ—Ä –ø—É–ª–∞ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è AI –æ–ø–µ—Ä–∞—Ü–∏–π
    ai-threads 4
    
    # –¢–∞–π–º–∞—É—Ç –¥–ª—è AI –æ–ø–µ—Ä–∞—Ü–∏–π
    ai-timeout 5000
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ AI –æ–ø–µ—Ä–∞—Ü–∏–π
    loglevel notice
    logfile /data/redis-ai.log
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    latency-monitor-threshold 100
    
    # –°–µ—Ç—å
    tcp-keepalive 300
    timeout 0
    
    # –ö–ª–∏–µ–Ω—Ç—ã
    maxclients 10000
    
    # –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    slowlog-log-slower-than 10000
    slowlog-max-len 128
```

## üîç AI Services Region: Managed AI —Å–µ—Ä–≤–∏—Å—ã

### SageMaker Integration - Managed ML –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞

#### SageMaker Endpoints –¥–ª—è production inference
```python
# infrastructure/sagemaker/model_deployment.py
import boto3
import json
from datetime import datetime
from typing import Dict, List, Optional

class SageMakerAIDeployment:
    """
    –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ SageMaker endpoints –¥–ª—è Auto.ru AI —Å–∏—Å—Ç–µ–º—ã
    """
    
    def __init__(self, region_name: str = 'us-east-1'):
        self.sagemaker = boto3.client('sagemaker', region_name=region_name)
        self.runtime = boto3.client('sagemaker-runtime', region_name=region_name)
        
    def deploy_performance_predictor(
        self, 
        model_data_url: str,
        instance_type: str = 'ml.c5.xlarge',
        instance_count: int = 2
    ) -> str:
        """
        –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        model_name = f"performance-predictor-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        endpoint_config_name = f"{model_name}-config"
        endpoint_name = f"{model_name}-endpoint"
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.sagemaker.create_model(
            ModelName=model_name,
            PrimaryContainer={
                'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.12.0-gpu-py38',
                'ModelDataUrl': model_data_url,
                'Environment': {
                    'SAGEMAKER_PROGRAM': 'inference.py',
                    'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/code',
                    'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',
                    'SAGEMAKER_REGION': 'us-east-1',
                    'MODEL_NAME': 'performance_predictor',
                    'BATCH_SIZE': '32',
                    'MAX_SEQUENCE_LENGTH': '512'
                }
            },
            ExecutionRoleArn='arn:aws:iam::ACCOUNT:role/SageMakerExecutionRole',
            Tags=[
                {'Key': 'Project', 'Value': 'auto-ru-ai'},
                {'Key': 'Component', 'Value': 'performance-predictor'},
                {'Key': 'Environment', 'Value': 'production'}
            ]
        )
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è endpoint
        self.sagemaker.create_endpoint_config(
            EndpointConfigName=endpoint_config_name,
            ProductionVariants=[
                {
                    'VariantName': 'primary',
                    'ModelName': model_name,
                    'InitialInstanceCount': instance_count,
                    'InstanceType': instance_type,
                    'InitialVariantWeight': 1.0,
                    'AcceleratorType': 'ml.eia2.medium'  # Elastic Inference –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                }
            ],
            DataCaptureConfig={
                'EnableCapture': True,
                'InitialSamplingPercentage': 10,
                'DestinationS3Uri': 's3://auto-ru-ml-data-capture/performance-predictor/',
                'CaptureOptions': [
                    {'CaptureMode': 'Input'},
                    {'CaptureMode': 'Output'}
                ]
            },
            Tags=[
                {'Key': 'Project', 'Value': 'auto-ru-ai'},
                {'Key': 'Component', 'Value': 'performance-predictor'}
            ]
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ endpoint
        self.sagemaker.create_endpoint(
            EndpointName=endpoint_name,
            EndpointConfigName=endpoint_config_name,
            Tags=[
                {'Key': 'Project', 'Value': 'auto-ru-ai'},
                {'Key': 'Component', 'Value': 'performance-predictor'},
                {'Key': 'AutoScaling', 'Value': 'enabled'}
            ]
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≤—Ç–æ–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
        self._setup_auto_scaling(endpoint_name, 'primary')
        
        return endpoint_name
    
    def deploy_multi_model_endpoint(
        self,
        models: List[Dict[str, str]],
        instance_type: str = 'ml.c5.2xlarge'
    ) -> str:
        """
        –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ multi-model endpoint –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö AI –º–æ–¥–µ–ª–µ–π
        """
        endpoint_name = f"auto-ru-multi-model-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        model_name = f"{endpoint_name}-model"
        endpoint_config_name = f"{endpoint_name}-config"
        
        # –°–æ–∑–¥–∞–Ω–∏–µ multi-model
        self.sagemaker.create_model(
            ModelName=model_name,
            PrimaryContainer={
                'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.12.0-gpu-py38',
                'Mode': 'MultiModel',
                'ModelDataUrl': 's3://auto-ru-ml-models/multi-model/',
                'Environment': {
                    'SAGEMAKER_PROGRAM': 'multi_model_inference.py',
                    'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/code',
                    'SAGEMAKER_MULTI_MODEL': 'true',
                    'SAGEMAKER_CONTAINER_LOG_LEVEL': '20'
                }
            },
            ExecutionRoleArn='arn:aws:iam::ACCOUNT:role/SageMakerExecutionRole'
        )
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        production_variants = []
        for i, model_config in enumerate(models):
            variant_name = model_config.get('name', f'variant-{i}')
            weight = model_config.get('weight', 1.0 / len(models))
            
            production_variants.append({
                'VariantName': variant_name,
                'ModelName': model_name,
                'InitialInstanceCount': 1,
                'InstanceType': instance_type,
                'InitialVariantWeight': weight
            })
        
        self.sagemaker.create_endpoint_config(
            EndpointConfigName=endpoint_config_name,
            ProductionVariants=production_variants,
            DataCaptureConfig={
                'EnableCapture': True,
                'InitialSamplingPercentage': 20,
                'DestinationS3Uri': 's3://auto-ru-ml-data-capture/multi-model/',
                'CaptureOptions': [
                    {'CaptureMode': 'Input'},
                    {'CaptureMode': 'Output'}
                ]
            }
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ endpoint
        self.sagemaker.create_endpoint(
            EndpointName=endpoint_name,
            EndpointConfigName=endpoint_config_name
        )
        
        return endpoint_name
    
    def _setup_auto_scaling(
        self, 
        endpoint_name: str, 
        variant_name: str,
        min_capacity: int = 1,
        max_capacity: int = 10
    ):
        """
        –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≤—Ç–æ–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è SageMaker endpoint
        """
        autoscaling = boto3.client('application-autoscaling')
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è scalable target
        autoscaling.register_scalable_target(
            ServiceNamespace='sagemaker',
            ResourceId=f'endpoint/{endpoint_name}/variant/{variant_name}',
            ScalableDimension='sagemaker:variant:DesiredInstanceCount',
            MinCapacity=min_capacity,
            MaxCapacity=max_capacity,
            RoleArn='arn:aws:iam::ACCOUNT:role/SageMakerAutoScalingRole'
        )
        
        # –ü–æ–ª–∏—Ç–∏–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω–≤–æ–∫–∞—Ü–∏–π
        autoscaling.put_scaling_policy(
            PolicyName=f'{endpoint_name}-invocations-scaling-policy',
            ServiceNamespace='sagemaker',
            ResourceId=f'endpoint/{endpoint_name}/variant/{variant_name}',
            ScalableDimension='sagemaker:variant:DesiredInstanceCount',
            PolicyType='TargetTrackingScaling',
            TargetTrackingScalingPolicyConfiguration={
                'TargetValue': 70.0,
                'PredefinedMetricSpecification': {
                    'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'
                },
                'ScaleOutCooldown': 300,
                'ScaleInCooldown': 300
            }
        )
        
        # –ü–æ–ª–∏—Ç–∏–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        autoscaling.put_scaling_policy(
            PolicyName=f'{endpoint_name}-latency-scaling-policy',
            ServiceNamespace='sagemaker',
            ResourceId=f'endpoint/{endpoint_name}/variant/{variant_name}',
            ScalableDimension='sagemaker:variant:DesiredInstanceCount',
            PolicyType='TargetTrackingScaling',
            TargetTrackingScalingPolicyConfiguration={
                'TargetValue': 100.0,  # 100ms target latency
                'PredefinedMetricSpecification': {
                    'PredefinedMetricType': 'SageMakerVariantModelLatency'
                },
                'ScaleOutCooldown': 180,
                'ScaleInCooldown': 300
            }
        )

# Terraform –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è SageMaker
# infrastructure/terraform/sagemaker.tf
resource "aws_sagemaker_model" "performance_predictor" {
  name               = "auto-ru-performance-predictor"
  execution_role_arn = aws_iam_role.sagemaker_execution_role.arn

  primary_container {
    image          = "763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.12.0-gpu-py38"
    model_data_url = "s3://auto-ru-ml-models/performance-predictor/model.tar.gz"
    
    environment = {
      SAGEMAKER_PROGRAM           = "inference.py"
      SAGEMAKER_SUBMIT_DIRECTORY  = "/opt/ml/code"
      MODEL_NAME                  = "performance_predictor"
      BATCH_SIZE                  = "32"
    }
  }

  tags = {
    Project     = "auto-ru-ai"
    Component   = "performance-predictor"
    Environment = "production"
  }
}

resource "aws_sagemaker_endpoint_configuration" "performance_predictor" {
  name = "auto-ru-performance-predictor-config"

  production_variants {
    variant_name           = "primary"
    model_name            = aws_sagemaker_model.performance_predictor.name
    initial_instance_count = 2
    instance_type         = "ml.c5.xlarge"
    initial_variant_weight = 1.0
    accelerator_type      = "ml.eia2.medium"
  }

  data_capture_config {
    enable_capture                = true
    initial_sampling_percentage   = 10
    destination_s3_uri           = "s3://auto-ru-ml-data-capture/performance-predictor/"
    
    capture_options {
      capture_mode = "Input"
    }
    
    capture_options {
      capture_mode = "Output"
    }
  }

  tags = {
    Project   = "auto-ru-ai"
    Component = "performance-predictor"
  }
}

resource "aws_sagemaker_endpoint" "performance_predictor" {
  name                 = "auto-ru-performance-predictor"
  endpoint_config_name = aws_sagemaker_endpoint_configuration.performance_predictor.name

  tags = {
    Project     = "auto-ru-ai"
    Component   = "performance-predictor"
    Environment = "production"
  }
}
```

## üéØ –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã Deployment –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### 1. Multi-Region AI Deployment
–ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏:
- **Edge AI**: CloudFront + Lambda@Edge –¥–ª—è inference –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ —Å–µ—Ç–∏
- **Regional Models**: –õ–æ–∫–∞–ª—å–Ω—ã–µ –∫–æ–ø–∏–∏ –º–æ–¥–µ–ª–µ–π –≤ –∫–∞–∂–¥–æ–º —Ä–µ–≥–∏–æ–Ω–µ
- **Global Model Registry**: –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏—è–º–∏ –º–æ–¥–µ–ª–µ–π

### 2. Auto-Scaling AI Workloads
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:
- **Predictive Scaling**: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
- **GPU Resource Management**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–æ—Ä–æ–≥–∏—Ö GPU —Ä–µ—Å—É—Ä—Å–æ–≤
- **Cost Optimization**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É instance —Ç–∏–ø–∞–º–∏

### 3. ML Model Lifecycle Management
–ü–æ–ª–Ω—ã–π –∂–∏–∑–Ω–µ–Ω–Ω—ã–π —Ü–∏–∫–ª ML –º–æ–¥–µ–ª–µ–π –≤ production:
- **Continuous Deployment**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥–µ–ø–ª–æ–π –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π –º–æ–¥–µ–ª–µ–π
- **A/B Testing**: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ production —Ç—Ä–∞—Ñ–∏–∫–µ
- **Rollback Capabilities**: –ë—ã—Å—Ç—Ä—ã–π –æ—Ç–∫–∞—Ç –∫ –ø—Ä–µ–¥—ã–¥—É—â–∏–º –≤–µ—Ä—Å–∏—è–º –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö

### 4. Observability –∏ Monitoring
–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ AI —Å–∏—Å—Ç–µ–º—ã:
- **Model Performance**: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
- **Infrastructure Metrics**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- **Business Impact**: –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è AI –Ω–∞ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏

### 5. Security –∏ Compliance
–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å AI —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω—è—Ö:
- **Model Security**: –ó–∞—â–∏—Ç–∞ ML –º–æ–¥–µ–ª–µ–π –æ—Ç –∞—Ç–∞–∫ –∏ —É—Ç–µ—á–µ–∫
- **Data Privacy**: –°–æ–±–ª—é–¥–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø–æ –∑–∞—â–∏—Ç–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **Access Control**: –†–æ–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–∞ –∫ AI —Ä–µ—Å—É—Ä—Å–∞–º

–≠—Ç–∞ Deployment –¥–∏–∞–≥—Ä–∞–º–º–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç enterprise-grade —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ AI-driven —Å–∏—Å—Ç–µ–º—ã, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–µ–µ –≤—ã—Å–æ–∫—É—é –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ production —Å—Ä–µ–¥–µ.